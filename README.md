<div align="center">

# ğŸš€ Slooze Data Engineering Take-Home Challenge

**End-to-End Data Pipeline: Web Scraping â†’ ETL â†’ EDA**

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)
![Selenium](https://img.shields.io/badge/Selenium-WebScraping-success?logo=selenium)
![Pandas](https://img.shields.io/badge/Pandas-DataCleaning-lightgrey?logo=pandas)
![Matplotlib](https://img.shields.io/badge/Matplotlib-Visualization-orange?logo=plotly)
![AWS](https://img.shields.io/badge/AWS-Ready-yellow?logo=amazon-aws)

</div>

---

## ğŸ“– **Overview**

This project is an **end-to-end data engineering solution** created for the **Slooze Data Engineer assessment**.
It demonstrates skills in:

* Dynamic data extraction using Selenium
* ETL pipeline design and data cleaning
* Exploratory Data Analysis (EDA) and visualization
* Building modular, scalable data workflows

---

## ğŸ§© **Key Features**

âœ… Dynamic data extraction using **Selenium**
âœ… Data cleaning and transformation via **Pandas (ETL)**
âœ… EDA & visualization with **Matplotlib**
âœ… Modular, reproducible folder structure
âœ… Ready for scaling to **AWS S3 / Glue / Airflow**

---

## ğŸ—‚ **Project Structure**

```bash
slz_takehome/
â”‚
â”œâ”€â”€ crawler/                         # Data collection layer
â”‚   â”œâ”€â”€ scraper_selenium.py          # Web scraper using Selenium
â”‚   â”œâ”€â”€ utils.py                     # Helper functions (headers, safe requests)
â”‚   â””â”€â”€ run_crawler.py               # Entry point for crawler
â”‚
â”œâ”€â”€ etl/                             # Data cleaning and transformation layer
â”‚   â””â”€â”€ transform.py                 # ETL script for data normalization
â”‚
â”œâ”€â”€ notebooks/                       # Data analysis and visualization
â”‚   â””â”€â”€ EDA.py                       # EDA and visualization script
â”‚
â”œâ”€â”€ output/                          # Data outputs
â”‚   â”œâ”€â”€ raw_products.json            # Raw scraped data
â”‚   â””â”€â”€ cleaned_products.csv         # Transformed structured dataset
â”‚
â”œâ”€â”€ plots/                           # Visualization outputs
â”‚   â”œâ”€â”€ min_price_hist.png           # Price distribution visualization
â”‚   â””â”€â”€ top_seller_cities.png        # Seller city frequency chart
â”‚
â”œâ”€â”€ README.md                        # Main project documentation (this file)
â”œâ”€â”€ REPORT.md                        # Summary report and insights
â””â”€â”€ requirements.txt                 # Python dependencies
```

---

## ğŸ§¬ **How to Run the Project**

---

### ğŸª„ **1ï¸âƒ£ Create and Activate Virtual Environment**

```bash
# Create a virtual environment
python -m venv venv

# Activate it (Windows PowerShell)
venv\Scripts\Activate.ps1
```

---

### âš™ï¸ **2ï¸âƒ£ Install Dependencies**

```bash
pip install -r requirements.txt
```

---

### ğŸš€ **3ï¸âƒ£ Run the Complete Pipeline**

#### ğŸ”¸ Step 1: Scrape Data from IndiaMART

```bash
python crawler\scraper_selenium.py
```

#### ğŸ— Step 2: Transform & Clean Data

```bash
python etl\transform.py
```

#### ğŸ“Š Step 3: Generate EDA Plots

```bash
python notebooks\EDA.py
```

---

### âœ… **Outputs**

| Type                | Description                   | File Path                                                 |
| ------------------- | ----------------------------- | --------------------------------------------------------- |
| ğŸ“¥ Raw scraped data | Extracted data from IndiaMART | `output/raw_products.json`                                |
| ğŸ¯¼ Cleaned dataset  | Transformed & normalized data | `output/cleaned_products.csv`                             |
| ğŸ“ˆ Charts           | Price & seller insights       | `plots/min_price_hist.png`, `plots/top_seller_cities.png` |

---

## ğŸ§¬ **Insights & Learnings**

* IndiaMART relies heavily on **JavaScript rendering**, requiring **Selenium** instead of static scraping.
* The **ETL pipeline** cleaned inconsistent and incomplete product data.
* EDA revealed **common price ranges and supplier concentration** across cities.
* Demonstrates how unstructured marketplace data can be turned into analytics-ready datasets.

---

## ğŸ§± **Future Enhancements**

| Area            | Enhancement                                                 |
| --------------- | ----------------------------------------------------------- |
| ğŸ”„ Pagination   | Handle multiple search pages for broader coverage           |
| â˜ï¸ Data Storage | Store cleaned data in **AWS S3**, query via **Athena/Glue** |
| ğŸ•’ Automation   | Schedule runs via **Apache Airflow** or **AWS Lambda**      |
| âœ… Validation    | Add data quality checks with **Great Expectations**         |
| âš¡ Scalability   | Migrate ETL to **PySpark** for larger-scale processing      |

---

## ğŸ“¦ **Submission Details**

| Field                  | Information                                                                               |
| ---------------------- | ----------------------------------------------------------------------------------------- |
| ğŸ¤© **Role**            | Data Engineer                                                                             |
| ğŸ¢ **Organization**    | Slooze                                                                                    |
| ğŸ“š **Deliverables**    | `raw_products.json`, `cleaned_products.csv`, EDA visualizations, `REPORT.md`, `README.md` |
| ğŸ‘¨â€ğŸ’» **Submitted by** | **Sanket Aba Adhav**                                                                      |

---

<div align="center">

âœ¨ *â€œTurning raw web data into meaningful insights â€” one pipeline at a time.â€* âœ¨
**Â© 2025 Sanket Aba Adhav â€” For Slooze Data Engineer Take-Home Challenge**

</div>
